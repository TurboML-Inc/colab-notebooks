{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Complex Stream Processing\n",
    "\n",
    "With real-time features, there can be situtations where the feature logic cannot be expressed by simple SQL, Aggregates or Scalar Python UDFs. In such scenarios, it may be required to write custom streaming pipelines. This is where TurboML is building on Ibis (https://github.com/ibis-project/ibis/), to expose a DataFrame like API to support complex streaming logic for features. We currently support Apache Flink and RisingWave backends for streaming execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment and install TurboML's SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "! conda install -qq conda-forge::libstdcxx-ng anaconda::protobuf\n",
    "! gdown -q 1UG8cmBLdYtEVJZG8Rgz454l722VAt_zE\n",
    "! unzip -qq turboml.zip -d turboml && pip install -qq turboml/*.whl && rm -rf turboml turboml.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to your TurboML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import turboml as tb\n",
    "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from turboml.common.sources import (\n",
    "    FileSource,\n",
    "    DataSource,\n",
    "    TimestampFormatConfig,\n",
    "    Watermark,\n",
    "    DataDeliveryMode,\n",
    "    S3Config,\n",
    ")\n",
    "from turboml.common.models import BackEnd\n",
    "import ibis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"data/transactions.csv\").reset_index()\n",
    "labels_df = pd.read_csv(\"data/labels.csv\").reset_index()\n",
    "labels = tb.PandasDataset(\n",
    "    dataset_name=\"labels_ibis_fe\", key_field=\"index\", dataframe=labels_df, upload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add feature definitions\n",
    "\n",
    "To add feature definitions, we have a class from turboml package called **IbisFeatureEngineering**. This allows us to define features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = tb.IbisFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the data for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install minio\n",
    "from minio import Minio\n",
    "\n",
    "client = Minio(\n",
    "    \"play.min.io\",\n",
    "    access_key=\"Q3AM3UQ867SPQQA43P2F\",\n",
    "    secret_key=\"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\",\n",
    "    secure=True,\n",
    ")\n",
    "bucket_name = \"ibis-demo\"\n",
    "found = client.bucket_exists(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not found:\n",
    "    client.make_bucket(bucket_name)\n",
    "    print(\"Created bucket\", bucket_name)\n",
    "else:\n",
    "    print(\"Bucket\", bucket_name, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.sql(\"SET s3_region='us-east-1';\")\n",
    "con.sql(\"SET s3_url_style='path';\")\n",
    "con.sql(\"SET s3_use_ssl=true;\")\n",
    "con.sql(\"SET s3_endpoint='play.min.io';\")\n",
    "con.sql(\"SET s3_access_key_id='Q3AM3UQ867SPQQA43P2F';\")\n",
    "con.sql(\"SET s3_secret_access_key='zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\n",
    "    \"COPY (SELECT * EXCLUDE(timestamp), TO_TIMESTAMP(CAST(timestamp AS DOUBLE)) AS timestamp FROM transactions_df) TO 's3://ibis-demo/transactions/transactions.parquet' (FORMAT 'parquet');\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSource\n",
    "The **DataSource** serves as the foundational entity in the feature engineering workflow. It defines where and how the raw data is accessed for processing. After creating a DataSource, users can register their source configurations to start leveraging them in the pipeline.\n",
    "\n",
    "#### Type of Delivery Modes\n",
    "1. Dynamic:\n",
    "    - Suitable for real-time or streaming data scenarios.\n",
    "    - Automatically creates connectors based on the source configuration.\n",
    "    - The Kafka topic becomes the primary input for feature engineering, ensuring seamless integration with downstream processing pipelines.\n",
    "2. Static:\n",
    "    - Designed for batch data sources.\n",
    "    - RisingWave/Flink reads directly from the source for feature engineering, eliminating the need for an intermediary Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col_config = TimestampFormatConfig(\n",
    "    format_type=TimestampFormatConfig.FormatType.EpochMillis\n",
    ")\n",
    "watermark = Watermark(\n",
    "    time_col=\"timestamp\", allowed_delay_seconds=60, time_col_config=time_col_config\n",
    ")\n",
    "ds1 = DataSource(\n",
    "    name=\"transactions_stream\",\n",
    "    key_fields=[\"index\"],\n",
    "    delivery_mode=DataDeliveryMode.DYNAMIC,\n",
    "    file_source=FileSource(\n",
    "        path=\"transactions\",\n",
    "        format=FileSource.Format.PARQUET,\n",
    "        s3_config=S3Config(\n",
    "            bucket=\"ibis-demo\",\n",
    "            region=\"us-east-1\",\n",
    "            access_key_id=\"Q3AM3UQ867SPQQA43P2F\",\n",
    "            secret_access_key=\"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\",\n",
    "            endpoint=\"https://play.min.io\",\n",
    "        ),\n",
    "    ),\n",
    "    watermark=watermark,\n",
    ")\n",
    "\n",
    "tb.register_source(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define features we can fetch the sources and perform operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = fe.get_ibis_table(\"transactions_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use one kafka topic (transactions_stream) to build features using Flink.\n",
    "\n",
    "We will also use UDF to define custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ibis.udf.scalar.python()\n",
    "def calculate_frequency_score(digital_count: float, physical_count: float) -> float:\n",
    "    if digital_count > 10 or physical_count > 10:\n",
    "        return 0.7  # High item count\n",
    "    elif digital_count == 0 and physical_count > 0:\n",
    "        return 0.3  # Physical item-only transaction\n",
    "    elif digital_count > 0 and physical_count == 0:\n",
    "        return 0.3  # Digital item-only transaction\n",
    "    else:\n",
    "        return 0.1  # Regular transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define features using ibis DSL or SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_frequency_score = transactions.select(\n",
    "    frequency_score=calculate_frequency_score(\n",
    "        transactions.digitalItemCount, transactions.physicalItemCount\n",
    "    ),\n",
    "    index=transactions.index,\n",
    "    digitalItemCount=transactions.digitalItemCount,\n",
    "    physicalItemCount=transactions.physicalItemCount,\n",
    "    transactionAmount=transactions.transactionAmount,\n",
    "    transactionTime=transactions.transactionTime,\n",
    "    isProxyIP=transactions.isProxyIP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview features locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_frequency_score.execute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After satisfied, we can materialize the features.\n",
    "It will write the features using flink.\n",
    "\n",
    "Flink uses a hybrid source to read first from iceberg table and then switches to kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.materialize_features(\n",
    "    transactions_with_frequency_score,\n",
    "    \"transactions_with_frequency_score\",\n",
    "    \"index\",\n",
    "    BackEnd.Flink,\n",
    "    \"transactions_stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a model using features built using flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tb.RCF(number_of_trees=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = [\"frequency_score\"]\n",
    "features = fe.get_input_fields(\n",
    "    \"transactions_with_frequency_score\", numerical_fields=numerical_fields\n",
    ")\n",
    "label = labels.get_label_field(label_field=\"is_fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_rcf = model.deploy(\n",
    "    name=\"demo_model_ibis_flink\", input=features, labels=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model_rcf.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = outputs[-1]\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([output[\"record\"].score for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoints = deployed_model_rcf.get_endpoints()\n",
    "model_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_query_datapoint = (\n",
    "    transactions_df[[\"index\", \"digitalItemCount\", \"physicalItemCount\"]]\n",
    "    .iloc[-1]\n",
    "    .to_dict()\n",
    ")\n",
    "model_query_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.post(\n",
    "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
    ")\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model_rcf.get_inference(transactions_df)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risingwave FE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now enrich the earlier built features using flink with features built using RisingWave.\n",
    "\n",
    "Let's fetch the features from server for the feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_frequency_score = fe.get_ibis_table(\n",
    "    \"transactions_with_frequency_score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ibis.udf.scalar.python()\n",
    "def detect_fraud(\n",
    "    transactionAmount: float, transactionTime: int, isProxyIP: float\n",
    ") -> int:\n",
    "    # Example logic for flagging fraud:\n",
    "    # - High transaction amount\n",
    "    # - Unusual transaction times (e.g., outside of working hours)\n",
    "    # - Use of proxy IP\n",
    "    is_high_amount = transactionAmount > 1000  # arbitrary high amount threshold\n",
    "    is_suspicious_time = (transactionTime < 6) | (\n",
    "        transactionTime > 22\n",
    "    )  # non-standard hours\n",
    "    is_proxy = isProxyIP == 1  # proxy IP flag\n",
    "\n",
    "    return int(is_high_amount & is_suspicious_time & is_proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_detection_expr = detect_fraud(\n",
    "    transactions_with_frequency_score.transactionAmount,\n",
    "    transactions_with_frequency_score.transactionTime,\n",
    "    transactions_with_frequency_score.isProxyIP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_fraud_flag = transactions_with_frequency_score.select(\n",
    "    transactionAmount=transactions_with_frequency_score.transactionAmount,\n",
    "    transactionTime=transactions_with_frequency_score.transactionTime,\n",
    "    isProxyIP=transactions_with_frequency_score.isProxyIP,\n",
    "    index=transactions_with_frequency_score.index,\n",
    "    digitalItemCount=transactions_with_frequency_score.digitalItemCount,\n",
    "    physicalItemCount=transactions_with_frequency_score.physicalItemCount,\n",
    "    frequency_score=transactions_with_frequency_score.frequency_score,\n",
    "    fraud_flag=fraud_detection_expr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_fraud_flag.execute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.materialize_features(\n",
    "    transactions_with_fraud_flag,\n",
    "    \"transactions_with_fraud_flag\",\n",
    "    \"index\",\n",
    "    BackEnd.Risingwave,\n",
    "    \"transactions_stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tb.RCF(number_of_trees=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = [\"frequency_score\", \"fraud_flag\"]\n",
    "features = fe.get_input_fields(\n",
    "    \"transactions_with_fraud_flag\", numerical_fields=numerical_fields\n",
    ")\n",
    "label = labels.get_label_field(label_field=\"is_fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_rcf = model.deploy(\n",
    "    name=\"demo_model_ibis_risingwave\", input=features, labels=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model_rcf.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = outputs[-1]\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([output[\"record\"].score for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoints = deployed_model_rcf.get_endpoints()\n",
    "model_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_query_datapoint = (\n",
    "    transactions_df[\n",
    "        [\n",
    "            \"index\",\n",
    "            \"digitalItemCount\",\n",
    "            \"physicalItemCount\",\n",
    "            \"transactionAmount\",\n",
    "            \"transactionTime\",\n",
    "            \"isProxyIP\",\n",
    "        ]\n",
    "    ]\n",
    "    .iloc[-1]\n",
    "    .to_dict()\n",
    ")\n",
    "model_query_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.post(\n",
    "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
    ")\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model_rcf.get_inference(transactions_df)\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
