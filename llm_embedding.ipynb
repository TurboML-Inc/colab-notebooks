{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LLM Embeddings\n",
    "\n",
    "One of the most important ways to model NLP tasks is to use pre-trained language model embeddings. This notebook covers how to download pre-trained models, use them to get text embeddings and build ML models on top of these embeddings using TurboML. We'll demonstrate this on a SMS Spam classification use-case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ae921",
   "metadata": {},
   "source": [
    "Clone the repo with notebooks and corresponding data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f587580",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TurboML-Inc/colab-notebooks.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d022b",
   "metadata": {},
   "source": [
    "Set up the environment and install TurboML's SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!bash colab-notebooks/install_turboml.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0044689",
   "metadata": {},
   "source": [
    "The kernel should now be restarted with TurboML's SDK installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd colab-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74035f21",
   "metadata": {},
   "source": [
    "Login to your TurboML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import turboml as tb\n",
    "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.SMSSpam()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list_x = []\n",
    "dict_list_y = []\n",
    "for x, y in dataset:\n",
    "    dict_list_x.append(x)\n",
    "    dict_list_y.append({\"label\": float(y)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame.from_dict(dict_list_x).reset_index()\n",
    "df_labels = pd.DataFrame.from_dict(dict_list_y).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    features = tb.PandasDataset(\n",
    "        dataset_name=\"sms_spam_features\",\n",
    "        key_field=\"index\",\n",
    "        dataframe=df_features,\n",
    "        upload=True,\n",
    "    )\n",
    "except:\n",
    "    features = tb.PandasDataset(dataset_name=\"sms_spam_features\")\n",
    "\n",
    "try:\n",
    "    labels = tb.PandasDataset(\n",
    "        dataset_name=\"sms_spam_labels\", key_field=\"index\", dataframe=df_labels, upload=True\n",
    "    )\n",
    "except:\n",
    "    labels = tb.PandasDataset(dataset_name=\"sms_spam_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = features.get_input_fields(textual_fields=[\"body\"])\n",
    "model_label = labels.get_label_field(label_field=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Downloading pre-trained models\n",
    "\n",
    "Huggingface Hub (https://huggingface.co/models) is one of the largest collection of pre-trained language models. It also has native intergrations with the GGUF format (https://huggingface.co/docs/hub/en/gguf). This format is quickly becoming the standard for saving and loading models, and popular open-source projects like llama.cpp and GPT4All use this format. TurboML also uses the GGUF format to load pre-trained models. Here's how you can specify a model from Huggingface Hub, and TurboML will download and convert this in the right format. \n",
    "\n",
    "We also support quantization of the model for conversion. The supported options are \"f32\", \"f16\", \"bf16\", \"q8_0\", \"auto\", where \"f32\" is for float32, \"f16\" for float16, \"bf16\" for bfloat16, \"q8_0\" for Q8_0, \"auto\" for the highest-fidelity 16-bit float type depending on the first loaded tensor type. \"auto\" is the default option. \n",
    "\n",
    "For this notebook, we'll use the https://huggingface.co/BAAI/bge-small-en-v1.5 model, with \"f16\" quantization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gguf_model = tb.acquire_hf_model_as_gguf(\"BAAI/bge-small-en-v1.5\", \"f16\")\n",
    "gguf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Once we have converted the pre-trained model, we can now use this to generate embeddings. Here's how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tb.LLAMAEmbedding(gguf_model_id=gguf_model)\n",
    "deployed_model = embedding_model.deploy(\n",
    "    \"bert_embedding\", input=model_features, labels=model_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model.get_outputs()\n",
    "embedding = outputs[0].get(\"record\").embeddings\n",
    "print(\n",
    "    \"Length of the embedding vector is:\",\n",
    "    len(embedding),\n",
    "    \". The first 5 values are:\",\n",
    "    embedding[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "But embeddings directly don't solve our use-case! We ultimately need a classification model for spam detection. We can build a pre-processor that converts all our text data into numerical embeddings, and then these numerical values can be passed to a classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tb.LlamaCppPreProcessor(base_model=tb.SGTClassifier(), gguf_model_id=gguf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = model.deploy(\n",
    "    \"bert_sgt_classifier\", input=model_features, labels=model_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = deployed_model.get_outputs()\n",
    "outputs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
