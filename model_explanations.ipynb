{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Explanations using iXAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The `iXAI` module can be used in combination with TurboML to provide incremental explanations for the models being trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We start by importing the `ixai` package and relevant datasets from `river`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33d387",
   "metadata": {},
   "source": [
    "Clone the repo with notebooks and corresponding data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TurboML-Inc/colab-notebooks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd colab-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d299ea1",
   "metadata": {},
   "source": [
    "Set up the environment and install TurboML's SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfe88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!bash install_turboml.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8b452",
   "metadata": {},
   "source": [
    "Login to your TurboML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import turboml as tb\n",
    "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ixai.explainer import IncrementalPFI\n",
    "from river.metrics import Accuracy\n",
    "from river.utils import Rolling\n",
    "from river.datasets.synth import Agrawal\n",
    "from river.datasets.synth import ConceptDriftStream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The sample size for the model to train on is defined.\n",
    "\n",
    "Also, we initialize a concept drift data stream using the `Agrawal` synthetic dataset from `river`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 150_000\n",
    "stream = Agrawal(classification_function=1, seed=42)\n",
    "drift_stream = Agrawal(classification_function=2, seed=42)\n",
    "stream = ConceptDriftStream(\n",
    "    stream,\n",
    "    drift_stream,\n",
    "    position=int(n_samples * 0.5),\n",
    "    width=int(n_samples * 0.1),\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list([x_0 for x_0, _ in stream.take(1)][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "A batch DataFrame is constructed from the stream defined above to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "for features, label in stream:\n",
    "    if len(features_list) == n_samples:\n",
    "        break\n",
    "    features_list.append(features)\n",
    "    labels_list.append(label)\n",
    "\n",
    "features_df = pd.DataFrame(features_list).reset_index()\n",
    "labels_df = pd.DataFrame(labels_list, columns=[\"label\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We use the `PandasDataset` class provided by TurboML to convert the DataFrame into a compatible dataset.\n",
    "\n",
    "As part of defining the dataset, we specify the column to be used for primary keys.\n",
    "\n",
    "Then, we get the relevant features from our dataset as defined by the `numerical_fields` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = tb.PandasDataset(\n",
    "    dataframe=features_df, key_field=\"index\", streaming=False\n",
    ")\n",
    "labels_full = tb.PandasDataset(dataframe=labels_df, key_field=\"index\", streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset_full.get_input_fields(numerical_fields=numerical_fields)\n",
    "label = labels_full.get_label_field(label_field=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We will be using and training the `Hoeffding Tree Classifier` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tb.HoeffdingTreeClassifier(n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learned = model.learn(features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Once the model has finished training, we get ready to deploy it so that it can be used for prediction.\n",
    "\n",
    "To begin with, we re-define our dataset to now support streaming data, and get the relevant features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = tb.PandasDataset(\n",
    "    dataset_name=\"agrawal_model_explanation\",\n",
    "    dataframe=features_df,\n",
    "    key_field=\"index\",\n",
    "    upload=True,\n",
    ")\n",
    "labels_full = tb.PandasDataset(\n",
    "    dataset_name=\"labels_model_explanation\",\n",
    "    key_field=\"index\",\n",
    "    dataframe=labels_df,\n",
    "    upload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset_full.get_input_fields(numerical_fields=numerical_fields)\n",
    "label = labels_full.get_label_field(label_field=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We specify that the model being deployed is to be used only for prediction using the `predict_only` parameter of the `deploy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = model_learned.deploy(\n",
    "    name=\"demo_model_ixai\", input=features, labels=label, predict_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Now, the `get_endpoints()` method is used to fetch an endpoint to which inference requests will be sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoints = deployed_model.get_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We define `model_function` as a wrapper for the inference requests being sent to the deployed model such that the outputs are compatible with `iXAI`'s explanations API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def model_function(x):\n",
    "    resp = requests.post(\n",
    "        model_endpoints[0], json=x, headers=tb.common.api.headers\n",
    "    ).json()\n",
    "    resp[\"output\"] = resp.pop(\"predicted_class\")\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We instantiate the `IncrementalPFI` class from `iXAI` with our prediction function defined above, along with the relevant fields from the dataset and the loss function to calculate the feature importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_pfi = IncrementalPFI(\n",
    "    model_function=model_function,\n",
    "    loss_function=Accuracy(),\n",
    "    feature_names=numerical_fields,\n",
    "    smoothing_alpha=0.001,\n",
    "    n_inner_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Finally, we loop through the stream for the first 10000 samples, updating our metric and `incremental_pfi` after each encountered sample. \n",
    "\n",
    "At every 1000th step, we print out the metric with the feature importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metric = Rolling(Accuracy(), window_size=1000)\n",
    "for n, (x_i, y_i) in enumerate(stream, start=1):\n",
    "    if n == 10000:\n",
    "        break\n",
    "\n",
    "    incremental_pfi.explain_one(x_i, y_i)\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print(\n",
    "            f\"{n}: Accuracy: {training_metric.get()} PFI: {incremental_pfi.importance_values}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
