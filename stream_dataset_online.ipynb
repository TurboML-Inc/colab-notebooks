{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Stream Dataset to Deployed Models\n",
    "This notebook demonstrates how to upload data to an already registered dataset with a deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56496114",
   "metadata": {},
   "source": [
    "Clone the repo with notebooks and corresponding data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TurboML-Inc/colab-notebooks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd colab-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e319e8e",
   "metadata": {},
   "source": [
    "Set up the environment and install TurboML's SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05684b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!bash install_turboml.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6144c",
   "metadata": {},
   "source": [
    "Login to your TurboML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c713874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import turboml as tb\n",
    "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.nb_utils import do_retry, simulate_realtime_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"data/transactions.csv\")\n",
    "labels_df = pd.read_csv(\"data/labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We will only use a subset of the dataset for initial model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transactions_df = transactions_df.iloc[0:20000]\n",
    "sub_transactions_df = sub_transactions_df.reset_index()\n",
    "sub_transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_labels_df = labels_df.iloc[0:20000]\n",
    "sub_labels_df = sub_labels_df.reset_index()\n",
    "sub_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset_id = \"transactions_stream_online\"\n",
    "transactions = tb.PandasDataset(\n",
    "    dataset_name=input_dataset_id,\n",
    "    key_field=\"index\",\n",
    "    dataframe=sub_transactions_df,\n",
    "    upload=True,\n",
    ")\n",
    "input_schema = transactions.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset_id = \"transaction_stream_labels\"\n",
    "labels = tb.PandasDataset(\n",
    "    dataset_name=label_dataset_id,\n",
    "    key_field=\"index\",\n",
    "    dataframe=sub_labels_df,\n",
    "    upload=True,\n",
    ")\n",
    "label_schema = labels.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.get_features(dataset_id=input_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Add feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.create_sql_features(\n",
    "    sql_definition='\"transactionAmount\" + \"localHour\"',\n",
    "    new_feature_name=\"my_sql_feat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.get_local_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.get_timestamp_formats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.register_timestamp(\n",
    "    column_name=\"timestamp\", format_type=\"epoch_seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.create_aggregate_features(\n",
    "    column_to_operate=\"transactionAmount\",\n",
    "    column_to_group=\"accountID\",\n",
    "    operation=\"SUM\",\n",
    "    new_feature_name=\"my_sum_feat\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    window_duration=24,\n",
    "    window_unit=\"hours\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.get_local_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Submit feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.feature_engineering.materialize_features([\"my_sql_feat\", \"my_sum_feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "materialized_features = transactions.feature_engineering.get_materialized_features()\n",
    "materialized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "htc_model = tb.HoeffdingTreeClassifier(n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = [\n",
    "    \"transactionAmount\",\n",
    "    \"localHour\",\n",
    "    \"my_sum_feat\",\n",
    "    \"my_sql_feat\",\n",
    "]\n",
    "categorical_fields = [\n",
    "    \"digitalItemCount\",\n",
    "    \"physicalItemCount\",\n",
    "    \"isProxyIP\",\n",
    "]\n",
    "features = transactions.get_input_fields(\n",
    "    numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
    ")\n",
    "label = labels.get_label_field(label_field=\"is_fraud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Run Supervised ML jobs\n",
    "We will deploy a HoeffdingTreeClassifier Model trained on a subset of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_htc = htc_model.deploy(\n",
    "    \"demo_classifier_htc_stream_model\", input=features, labels=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = do_retry(\n",
    "    deployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Supervised Model Endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoints = deployed_model_htc.get_endpoints()\n",
    "model_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_query_datapoint = transactions_df.iloc[765].to_dict()\n",
    "model_query_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(\n",
    "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
    ")\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Supervised Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_htc.add_metric(\"WindowedAUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc_scores = do_retry(\n",
    "    deployed_model_htc.get_evaluation,\n",
    "    \"WindowedAUC\",\n",
    "    return_on=(lambda result: len(result) > 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Upload to dataset with online model\n",
    "We will upload data to the registered dataset, which will be used for training and inference by the respective deployed model in realtime. \n",
    "\n",
    "We use a helper function `simulate_realtime_stream` from `utils/nb_utils.py` to simulate realtime streaming data from dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Upload using SDK\n",
    "Here we use the **upload_df** method provided by the **PandasDataset** class to upload data to a registered dataset. This method internally uploads the data using the **Arrow Flight Protocol** over gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transactions_df = transactions_df.iloc[20000:100000]\n",
    "sub_transactions_df = sub_transactions_df.reset_index()\n",
    "sub_transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_labels_df = labels_df.iloc[20000:100000]\n",
    "sub_labels_df = sub_labels_df.reset_index()\n",
    "sub_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Set the chunk size and delay for the `simulate_realtime_stream` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10 * 1024\n",
    "delay = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Here we zip the two stream generators to get a batch of dataframe for input and label datasets and we upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\n",
    "realtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n",
    "\n",
    "with tqdm(\n",
    "    total=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n",
    ") as pbar:\n",
    "    for input_stream, label_stream in zip(\n",
    "        realtime_input_stream, realtime_label_stream, strict=True\n",
    "    ):\n",
    "        start = time.perf_counter()\n",
    "        transactions.upload_df(dataframe=input_stream)\n",
    "        labels.upload_df(dataframe=label_stream)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        pbar.update(len(input_stream))\n",
    "        print(\n",
    "            f\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Check Updated Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.get_features(dataset_id=input_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "We can use the **sync_features** method to sync the materialized streaming features to the **PandasDataset** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "transactions.sync_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Calling **get_materialized_features** method will show that newly uploaded data is properly materialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "materialized_features = transactions.feature_engineering.get_materialized_features()\n",
    "materialized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The **get_ouputs** method will return the latest processed ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = do_retry(\n",
    "    deployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n",
    ")\n",
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\n",
    "print(len(model_auc_scores))\n",
    "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(\n",
    "    model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n",
    ")\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Upload using REST API\n",
    "Here we use the **dataset/dataset_id/upload** REST API endpoint to upload data to a registered dataset. This endpoint will directly upload the data to the registered **dataset kafka topic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transactions_df = transactions_df.iloc[100000:170000]\n",
    "sub_transactions_df = sub_transactions_df.reset_index()\n",
    "sub_transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_labels_df = labels_df.iloc[100000:170000]\n",
    "sub_labels_df = sub_labels_df.reset_index()\n",
    "sub_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turboml.common.api import api\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "We use the turboml api module to initiate the HTTP call, since auth is already configured for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_upload_df(dataset_id: str, df: pd.DataFrame):\n",
    "    row_list = json.loads(df.to_json(orient=\"records\"))\n",
    "    api.post(f\"dataset/{dataset_id}/upload\", json=row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\n",
    "realtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n",
    "\n",
    "with tqdm(\n",
    "    total=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n",
    ") as pbar:\n",
    "    for input_stream, label_stream in zip(\n",
    "        realtime_input_stream, realtime_label_stream, strict=True\n",
    "    ):\n",
    "        start = time.perf_counter()\n",
    "        rest_upload_df(input_dataset_id, input_stream)\n",
    "        rest_upload_df(label_dataset_id, label_stream)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        pbar.update(len(input_stream))\n",
    "        print(\n",
    "            f\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "#### Check Updated Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "transactions.sync_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "materialized_features = transactions.feature_engineering.get_materialized_features()\n",
    "materialized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = do_retry(\n",
    "    deployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\n",
    "print(len(model_auc_scores))\n",
    "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_htc.get_inference(transactions_df.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Upload using gRPC API \n",
    "This example shows how to directly upload data to the registered dataset using Arrow Flight gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transactions_df = transactions_df.iloc[170000:]\n",
    "sub_transactions_df = sub_transactions_df.reset_index()\n",
    "sub_transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_labels_df = labels_df.iloc[170000:]\n",
    "sub_labels_df = sub_labels_df.reset_index()\n",
    "sub_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import struct\n",
    "import itertools\n",
    "from functools import partial\n",
    "from pyarrow.flight import FlightDescriptor\n",
    "\n",
    "from turboml.common.env import CONFIG as tb_config\n",
    "from turboml.common import get_protobuf_class, create_protobuf_from_row_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Here we have defined a helper function `write_batch` to write pyarrow record batch given a pyarrow flight client instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch(writer, df, proto_gen_partial_func):\n",
    "    row_iter = df.itertuples(index=False, name=None)\n",
    "    batch_size = 1024\n",
    "    while True:\n",
    "        batch = list(\n",
    "            map(\n",
    "                proto_gen_partial_func,\n",
    "                itertools.islice(row_iter, batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        batch = pyarrow.RecordBatch.from_arrays([batch], [\"value\"])\n",
    "        writer.write(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "We initiate connection for the pyarrow flight client to the TurboML arrow server with the required configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_server_grpc_endpoint = tb_config.ARROW_SERVER_ADDRESS\n",
    "\n",
    "# Note: SchemaId prefix is required for proper kafka protobuf serialization.\n",
    "input_proto_gen_func = partial(\n",
    "    create_protobuf_from_row_tuple,\n",
    "    fields=sub_transactions_df.columns.tolist(),\n",
    "    proto_cls=get_protobuf_class(input_dataset_id, input_schema.schema_body),\n",
    "    prefix=struct.pack(\"!xIx\", input_schema.id),\n",
    ")\n",
    "\n",
    "label_proto_gen_func = partial(\n",
    "    create_protobuf_from_row_tuple,\n",
    "    fields=sub_labels_df.columns.tolist(),\n",
    "    proto_cls=get_protobuf_class(label_dataset_id, label_schema.schema_body),\n",
    "    prefix=struct.pack(\"!xIx\", label_schema.id),\n",
    ")\n",
    "\n",
    "client = pyarrow.flight.connect(arrow_server_grpc_endpoint)\n",
    "# Note: Expected arrow schema is a column named 'value' with serialized protobuf binary message.\n",
    "pa_schema = pyarrow.schema([(\"value\", pyarrow.binary())])\n",
    "\n",
    "input_stream_writer, _ = client.do_put(\n",
    "    FlightDescriptor.for_command(f\"produce:{input_dataset_id}\"),\n",
    "    pa_schema,\n",
    "    options=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n",
    ")\n",
    "\n",
    "label_stream_writer, _ = client.do_put(\n",
    "    FlightDescriptor.for_command(f\"produce:{label_dataset_id}\"),\n",
    "    pa_schema,\n",
    "    options=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Now, we use the stream generator and pass the data to the `write_batch` function along with **pyarrow client write handler** for for both input and label data writes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\n",
    "realtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n",
    "\n",
    "with tqdm(\n",
    "    total=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n",
    ") as pbar:\n",
    "    for input_stream, label_stream in zip(\n",
    "        realtime_input_stream, realtime_label_stream, strict=True\n",
    "    ):\n",
    "        start = time.perf_counter()\n",
    "        write_batch(input_stream_writer, input_stream, input_proto_gen_func)\n",
    "        write_batch(label_stream_writer, label_stream, label_proto_gen_func)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        pbar.update(len(input_stream))\n",
    "        print(\n",
    "            f\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Close the pyarrow client write handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_stream_writer.close()\n",
    "label_stream_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### Check Updated Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "transactions.sync_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "materialized_features = transactions.feature_engineering.get_materialized_features()\n",
    "materialized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = do_retry(\n",
    "    deployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\n",
    "print(len(model_auc_scores))\n",
    "plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
