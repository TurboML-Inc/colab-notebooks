{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TurboML LLM Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TurboML can spin up LLM servers with an OpenAI-compatible API. We currently support models\n",
    "in the GGUF format, but also support non-GGUF models that can be converted to GGUF. In the latter\n",
    "case you get to decide the quantization type you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repo with notebooks and corresponding data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TurboML-Inc/colab-notebooks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd colab-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment and install TurboML's SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!bash install_turboml.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to your TurboML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import turboml as tb\n",
    "tb.init(backend_url=BACKEND_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlamaServerRequest = tb.llm.LlamaServerRequest\n",
    "HuggingFaceSpec = LlamaServerRequest.HuggingFaceSpec\n",
    "ServerParams = LlamaServerRequest.ServerParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a model\n",
    "Let's use a Llama 3.2 quant already in the GGUF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_spec = HuggingFaceSpec(\n",
    "    hf_repo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "    select_gguf_file=\"llama-3.2-3b-instruct-q8_0.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn a server\n",
    "On spawning a server, you get a `server_id` to reference it later as well as `server_relative_url` you can\n",
    "use to reach it. This method is synchronous, so it can take a while to yield as we retrieve (and convert) your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tb.llm.spawn_llm_server(\n",
    "    LlamaServerRequest(\n",
    "        source_type=LlamaServerRequest.SourceType.HUGGINGFACE,\n",
    "        hf_spec=hf_spec,\n",
    "        server_params=ServerParams(\n",
    "            threads=-1,\n",
    "            seed=-1,\n",
    "            context_size=0,\n",
    "            flash_attention=False,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_id = response.server_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the LLM\n",
    "\n",
    "Our LLM is exposed with an OpenAI-compatible API, so we can use the OpenAI SDK, or any\n",
    "other tool compatible tool to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url = tb.common.env.CONFIG.TURBOML_BACKEND_SERVER_ADDRESS\n",
    "server_url = f\"{base_url}/{response.server_relative_url}\"\n",
    "\n",
    "client = OpenAI(base_url=server_url, api_key=\"-\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello there how are you doing today?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"-\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.llm.stop_llm_server(server_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
